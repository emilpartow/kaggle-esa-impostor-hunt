# 2nd Place Solution: Pairwise Features + LLM Signals + LightGBM

This repository contains my 2nd place solution to [Fake or Real: The Impostor Hunt – Kaggle Competition](https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt).
The core idea was to treat the task as a tabular classification problem on text pairs, combining classical linguistic features with zero-shot judgments from a large language model (LLM). The final prediction was made using a LightGBM classifier.

---

## Key Takeaway

Since the manipulated texts were themselves generated by a large model, strong LLMs are naturally well-suited to detect subtle stylistic deviations. Zero-shot judgments encoded semantic priors that were hard to obtain from raw features alone. However, they became most effective only when distilled into structured numeric features and passed to a supervised learner (LightGBM).

---
## Repository Structure

```
├── data/                              # train/test data 
├── src/
│   ├── dataset.py                     # Pair construction and Data Preprocessing
│   ├── config.yaml                    # Set Api-Key  
│   ├── basic_feature_engineering.py   # Classical Text Comparison utils
│   └── grok.py                        # Calls LLM API and stores features
│   
├── notebooks/
│   ├── 00_add_features.ipynb          # Full feature pipeline
│   └── 01_train.ipynb                 # Training + validation
│ 
└── README.md
```

---

## Running the Code

```bash
# Install dependencies
pip install -r requirements.txt

# (Optional) Add your Grok/LLM API key to config.yaml

# Use notebooks
jupyter notebook notebooks/00_add_features.ipynb
jupyter notebook notebooks/01_train.ipynb
```

---

# More Details

## Overall Pipeline

```
Pairwise raw texts
     ⬇
Classical single-text + pairwise features
     ⬇
LLM judgments (converted to numeric scores)
     ⬇
LightGBM binary classifier ("B is fake" vs "A is fake")
     ⬇
Submission: real_text_id ∈ {1, 2}
```

---

## Classical Feature Engineering

For each text (A and B), I extracted a set of lightweight, explainable features such as:
- Word / character / sentence counts  
- Flesch reading ease  
- Average word length  
- Number of digits or uppercase words  
- Longest sentence length  
- Repeated token heuristics  
- Punctuation endings  

Then I created pairwise interaction features:
- Differences between A and B (e.g. `flesch_A - flesch_B`)  
- TF-IDF cosine similarity between the two texts  
- Token-level Jaccard similarity  

These features already captured relevant stylistic differences between “real” and “impostor” texts.

---

## LLM Judgments (Used as Features)

I used **Grok**, as it was freely accessible during the competition. In principle, any strong LLM (ChatGPT, Claude, Gemini, etc.) could be substituted.

For each pair:
```json
{
  "fake": "A" or "B",
  "scores": {"A": float, "B": float}
}
```

To reduce variance:
1. Single-shot judgment  
2. “Double precision” re-evaluation  
3. Tie-break if outputs disagreed  

All outputs were treated **only as numeric features**.
The classifier then learned how to trust (or ignore) these signals in combination with the classical features.

---

## Model Training (LightGBM)

I converted the task into:  
**Target = 1 if B is fake, else 0**.

Trained using classical + LLM features:
```python
model = lgb.LGBMClassifier(
    n_estimators=2000,
    learning_rate=0.02,
    num_leaves=63,
    subsample=0.9,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=41
)
model.fit(...)
```

A stratified validation split correlated well with leaderboard performance.

---

## Acknowledgements

Thanks to the organizers for a fantastic challenge and the community for inspiration. 
